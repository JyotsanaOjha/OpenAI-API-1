{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "980c31b4-26e5-47c0-9b0f-a06d81f0289d",
   "metadata": {},
   "source": [
    "Two types of cells:1)code cells(to write python code) 2)markdown cells(Markdown is a markup language that is superset of HTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea4fe19-28f5-445c-aa27-4e9726fdf326",
   "metadata": {},
   "source": [
    "#### Markdown Cell Text Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cefc53-863c-48fe-a484-3c4a79b5495d",
   "metadata": {},
   "source": [
    "To create heading text add 1 to 6 # before your heading text.as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487d5d7-8a12-4cf6-aefc-de828a690e06",
   "metadata": {},
   "source": [
    "## heading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85329e-a296-4291-ad36-8f4587bc4965",
   "metadata": {},
   "source": [
    "**This is bold.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff56037-4b4c-432e-bbd5-ae4925152ca5",
   "metadata": {},
   "source": [
    "*this is italic.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e79d6-c657-4f8c-ad93-9b1cee5d78f0",
   "metadata": {},
   "source": [
    "Piece of code within a sentence using single backticks. as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f8472-7805-4b27-8692-15ce80179513",
   "metadata": {},
   "source": [
    "`append()`is a list method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8c158b-dee5-4619-a6ab-b74e97a0fb1b",
   "metadata": {},
   "source": [
    "Create an unordered list as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d833a4-acd0-46e9-a2c1-f19ab43485be",
   "metadata": {},
   "source": [
    "Python Methods:\n",
    "- append()\n",
    "- insert()\n",
    "- pop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd480069-be37-46b4-9158-c232f3cff4bf",
   "metadata": {},
   "source": [
    "#### Code Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4b0fb5-1ff8-4aed-a908-6955c53f9631",
   "metadata": {},
   "source": [
    "If I declare a variable in one cell then that variable will also exist in the other cells of the same notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb23d4ca-fbec-4055-b2fb-adbaf50c0111",
   "metadata": {},
   "source": [
    "`help(ChatOpenAI)`   :-it helps us to find details(like temperature or the llm model used etc) of any class\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd9dd95-6d23-4ecd-9459-18d3723ad4e0",
   "metadata": {},
   "source": [
    "### Installing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5b73b-9e1c-463d-980f-3e66a62e007a",
   "metadata": {},
   "source": [
    "To install the libraries we can use following command:\n",
    " **pip install `<name of library>`**\n",
    "but if there are multiple libraries to be installed then there is a trick. Make a file named requirements.txt in the project directory. This file has the names of all the libraries to be installed.\n",
    "Now run the following command: \n",
    " **pip install -r ./requirements.txt -q**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f84b608-cee2-4370-9b2d-7d51c51c7b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2f20f7-7b72-4001-b8fe-c8476d9f4595",
   "metadata": {},
   "source": [
    "### Python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0bbfad-b832-49c1-8c8a-a6b5a0417860",
   "metadata": {},
   "source": [
    "python-dotenv is a Python Module that allows you to specify environment variables as key-value pairs in a .env file within your Python project directory. It is a convenient and secure way to load and use environment variables in your application.We can have more than one API Key and will save them all in .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83bb56b1-1be9-4067-b654-7a143fe36d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the environment variables.\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)                  \n",
    "\n",
    "\n",
    "\n",
    "#os.environ.get('OPENAI_API_KEY') :- to get the value of an environment variable:\n",
    "\n",
    "\n",
    "#load_dotenv()  :- To load the variables found in .env file. Here we can give the path to the .env file as an argument  OR  we can call find_dotenv(). \n",
    "#find_dotenv()  :- tries to find the .env file.It returns a path to the file, if found or an empty string otherwise. \n",
    "#override=True  :- It is necessary to override the value of the variable if you change it in the .env file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba6b72-e90b-4bf2-8187-15b1ecb469d0",
   "metadata": {},
   "source": [
    "## ChatModels: GTP-3.5-Turbo and GPT-4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a322a3-a1f3-4d0b-ac10-7b5efccd869b",
   "metadata": {},
   "source": [
    "The core element of any LLM application is the AI model. Langchain doesn't have its own LLMs, but it provides a standard interface for interacting with various LLM's, including OpenAI's GPT models, google's gemini, Meta's Llama, and more. In Langchain Terminology, These are called LLM providers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e2b67-73ff-45a7-b28c-6f0f1a7d819b",
   "metadata": {},
   "source": [
    "### Invoke OpenAI's GPT models(ex: gpt-4-turbo-preview, gpt-3.5-turbo etc.) within LangChain\n",
    "These models expose an interface where chat messages or conversations serve as inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e03ffe43-f342-42c2-b3fd-bedcec8e4281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "React.js is a popular JavaScript library used for building user interfaces. Developed by Facebook, React allows developers to create dynamic, interactive web applications with ease. It uses a component-based architecture, where each part of the user interface is broken down into reusable components. This makes it easier to manage and update code, as well as improve performance. React also utilizes a virtual DOM, which helps minimize the number of updates needed to the actual DOM, resulting in faster rendering speeds. Overall, React.js is a powerful tool for building modern, efficient web applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "#help(ChatOpenAI())\n",
    "\n",
    "prompt= \"Tell me about React.js in 100 words\"\n",
    "output = llm.invoke(prompt)\n",
    "# If you want to use use 'gpt-4-turbo-preview' instead of default(gpt-3.5-turbo) model then add a new argument to invoke() method. As follows:\n",
    "# output = llm.invoke(prompt, model='gpt-4-turbo-preview')\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5dd569-0b41-4d5c-9daa-e83d19e17fd4",
   "metadata": {},
   "source": [
    "OpenAI **Chat Completions API** (took from the documentation on the OpenAI website )check below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a100a842-9547-4b41-8eec-4dd55d50fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "\n",
    "# response = client.chat.completions.create(\n",
    "#   model=\"gpt-3.5-turbo\",\n",
    "#    messages=[\n",
    "#     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "#   ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4e4bd7-0bed-466b-8fe3-55adfd2e1caf",
   "metadata": {},
   "source": [
    "The OpenAI API for GPT-3.5-turbo, GPT-4, GPT-4-turbo and so on... uses a list of dictionaries called \"messages\". These messages define three roles 1)system:- It helps set the behavior of the assistant.   2)user:- It is the prompt or the question we ask the assistant.  3)Assistant:- It stores prior responses. To use these message--> when calling the LLM from Langchain, we need to import a schema from langchain.schema for the messages. Here we import three classes for the three message types. 1)SystemMessage(corresponds to the \"system\" message in the \"OpenAI Chat Completion API\"), 2)AIMessage(equivalent to the \"assistant\" message in the \"OpenAI Chat Completion API\") and 3)HumanMessage(Represents the \"user\" message or prompt in the \"OpenAI Chat Completion API\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c6b099-d8b3-4166-8fc4-486f84923dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "गुरुत्वाकर्षण का नियम विश्वास के अनुसार उस शक्ति को संदर्भित करता है जो एक वस्तु को दूसरी वस्तु की ओर खींचने के लिए जिम्मेदार है। इस नियम के अनुसार, हर दो वस्तुओं के बीच मास और दूरी के आधार पर एक गुरुत्वाकर्षण बल होता है जो उन्हें एक-दूसरे की ओर खींचता है। इस क्रिया के लिए गुरुत्वाकर्षण सूत्र को न्यूटन का गुरुत्वाकर्षण कानून कहा जाता है।\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import(\n",
    "SystemMessage,\n",
    "AIMessage,\n",
    "HumanMessage)\n",
    "\n",
    "#creating a list named messages\n",
    "messages=[\n",
    "         SystemMessage(content='you are a physicist and responds only in Hindi.'),\n",
    "         HumanMessage(content='Explain the law of gravity.')\n",
    "         \n",
    "         ]\n",
    "\n",
    "output=llm.invoke(messages)\n",
    "print(output.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
