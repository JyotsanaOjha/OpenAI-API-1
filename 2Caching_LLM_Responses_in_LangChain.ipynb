{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a9671fb-85fe-4638-9dfd-96a62cef6ac3",
   "metadata": {},
   "source": [
    "# Caching LLM Responses in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c836c27-ee44-4e29-a86a-5ec5644b1d43",
   "metadata": {},
   "source": [
    "Caching is the practice of storing frequently accessed data or results in a temporary, faster storage layer.\n",
    "\n",
    "In the context of LangChain, Caching optimizes interactions with LLMs by reducing API calls and speeding up applications, resulting in a more efficient user experience.\n",
    "\n",
    "when we repeatedly request the same completion from LLM, Caching ensures that the result is stored locally. Subsequent requests for the same input can then be served directly from the cache, reducing the number of expensive API Calls to the LLM provider by avoiding redundant API calls.\n",
    "\n",
    "Two types of caching: **1)In-Memory Caching** and **2)SQLite Caching**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fde425f-10a9-4f4c-9f36-34dc79120f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r ./requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d986fecd-67d7-40a2-b927-9fba909f9665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ccc0b1-6cc5-491e-b603-ce23916ddc51",
   "metadata": {},
   "source": [
    "## 1)In-Memory Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "673e2985-f6e5-4f0e-b277-8f99b72072f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb34da06-319f-4fa3-9043-555f67fab94f",
   "metadata": {},
   "source": [
    "To Measure the response time of the model use **`%%time`**. This command in Jupyter Notebook is used to measure the execution time of the code within the current cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "404dde81-808b-4fe3-8cee-2049b76c5e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 681 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy was the math book sad? Because it had too many problems!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "prompt = 'Tell me a joke that a toddler can understand.'\n",
    "\n",
    "#following request is not in the cache so it will take longer. \n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cdc7e57-0105-4a80-ba3b-442ca4bf89e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy was the math book sad? Because it had too many problems!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#This time the request is already in the cache so it will be faster\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12558553-4a97-4acf-b823-fd52674a0cf3",
   "metadata": {},
   "source": [
    "## 2)SQLite Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aca5c0d0-0cba-48f4-9d48-8101cf23200a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 773 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chocolate chip cookie go to the doctor?\\n\\nBecause it was feeling crumbly!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n",
    "\n",
    "prompt2 = 'Tell me a joke about chocolate that a toddler can understand.'\n",
    "\n",
    "# The following request is not in the cache so it will take longer. \n",
    "llm.invoke(prompt2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e87410fa-cccb-4723-b718-bd9ca5293af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 967 Âµs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nWhy did the chocolate chip cookie go to the doctor?\\n\\nBecause it was feeling crumbly!'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#This time the request is already in the cache so it will be faster\n",
    "llm.invoke(prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1839c623-02d3-40f4-b5c0-b21c2f183af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
