{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f8661e-296e-44fe-8b48-de952240b5e1",
   "metadata": {},
   "source": [
    "# Introduction of VectorDatabases, Embeddings, and Pinecone\n",
    "**OPL Stack: OpenAI, Pinecone, and LangChain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a283b99-3516-4251-840f-2c3c8eb7bca8",
   "metadata": {},
   "source": [
    "**Embeddings** are the core of building LLM applications. \n",
    "\n",
    "Text embeddings are numeric representations of text, used in NLP(natural language processing) and ML(machine learning) tasks. Text embeddings can be used to measure the relatedness and similarity between two pieces of text. Relatedness measures how closely two pieces of text are related in meaning.\n",
    "\n",
    "The distance between two embeddings or two vectors measures their relatedness which translates to the relatedness between the text concepts they represent. Similar embeddings or vectors represent similar concepts. Text concepts are words and phrases. Similar embeddings or vectors represent similar concepts.\n",
    "\n",
    "**There are two common approaches to measure relatedness and similarity between text embeddings.**\n",
    "**1)Cosine similarity and\n",
    "  2)Euclidean distance**\n",
    "\n",
    "  #### Embeddings Applications:\n",
    "  ##### 1) Text Classification:\n",
    "  Assigning a label to a piece of text.\n",
    "\n",
    "  ##### 2) Text Clustering:\n",
    "  Grouping together pieces of text that are similar in meaning.\n",
    "\n",
    "  ##### 3) Question-Answering: \n",
    "  Answering a question posed in natural language.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13c4cad-8892-4be2-a105-4c7a9fde0c14",
   "metadata": {},
   "source": [
    "## Vector Databases  \n",
    "(**OPL Stack for developing LLM applications:** The OPL stack is a set of open-source tools that can be used to build applications that use \"Large Language Models\". **The stack consists of three main components OpenAI, Pinecone, and LangChain**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c792a9a-9cd0-4fb6-9992-fad1fa87ea3e",
   "metadata": {},
   "source": [
    "One of the biggest challenges of AI applications is efficient data processing. AI applications such as LLMs, Generative AI, and semantic search require a large amount of data to train and operate. Efficient data processing is essential for making AI applications successful.\n",
    "\n",
    "Many of the latest AI applications(ex. chatbots, question-answering systems, and machine translation) rely on vector embeddings.**Vector Embeddings means converting text to numbers that carry within themselves semantic information.** Vector Embeddings are a way to represent text as a set of numbers in a high dimensional space, and the numbers represent the meaning of the words in the text.\n",
    "\n",
    "We need a specialized database or data store specifically designed to manage large quantities of data in a numeric representation. There are many vector databases available, both free and commercial. **Examples: Pinecone, Chroma, Milvus, qdrant.**\n",
    "\n",
    "Pinecone is a vector database designed for storing and quering high dimensional vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0abe34-6684-44af-9279-e6be16cf9d96",
   "metadata": {},
   "source": [
    "**Vector Databases are a new type of database, designed to store and query unstructured data.**\n",
    "\n",
    "**Unstructured data is data that does not have a fixed schema, such as text, images, and audio.**\n",
    "\n",
    "vector databases are much more efficient at storing and querying unstructured data and are heavily used in LLMs applications.\n",
    "\n",
    "Just like the \"select\" statement in the SQL, in Vector databases we apply a similarity metric to find a vector that is the most similar to our query. Vector databases use a combination of different optimized algorithms that all participate in **Approximate Nearest Neighbor**(ANN) search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037f34d7-5970-4dea-a60a-c6ad3afdfa24",
   "metadata": {},
   "source": [
    "## How does vector database work?\n",
    "Three steps:\n",
    "\n",
    "**1)Embedding:-** Create vector embeddings for the content we want to index. This is done by using an embedding model such as Openai's \"text-embedding-ada-002\" or \"text-embedding-3-small\"\n",
    "\n",
    "**2)Indexing:-** Insert the vector embeddings into the vector database. This is done by associating each vector embedding with a reference to the original content used to create it.\n",
    "\n",
    "**3)Querying:-** Query the vector database for similar content. This is done using the same embedding model used to create the vector embeddings. \n",
    "\n",
    "The **embedding model** is used to create a vector embedding for the query, and this vector embedding is then used to query the database for similar vector embeddings. The similar vector embeddings are then associated with the original content that was used to create them.\n",
    "\n",
    "example:- Let's imagine a company that wants to store and query its private documents. The company would first use an embedding model to create vector embeddings for each document. These vector embeddings would then be inserted into a vector database such as Pinecone. Each vector embedding would be associated with a reference to the original document. Finally, the company can use the vector database to query for information that is similar to a given query or question. for instance, the company could query the documents or a product description."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd778044-d82f-4fea-8cea-2e4b283e9b28",
   "metadata": {},
   "source": [
    "## Pinecone (example of vector database)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb6d04-781d-4d3c-8e34-11ee9157e125",
   "metadata": {},
   "source": [
    "Pinecone is a high-performance, scalable and distributed vector store designed for LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be741a6e-3306-43b8-818b-0c6af3971391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a9b2a-308d-4eb5-b9aa-7654e5a5a493",
   "metadata": {},
   "source": [
    "To install the pinecone-client library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67937a3e-951c-4678-991e-572527766c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q pinecone-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e29c7aa-2357-44c8-ade5-713724d1a17a",
   "metadata": {},
   "source": [
    "To upgrade the pinecone-client library to its latest version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89069a09-381f-4eac-847b-d1e59f7fd75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade -q pinecone-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd0c8e3-6b05-40b5-9d6a-7ce288fe8dfb",
   "metadata": {},
   "source": [
    "To display the version of the installed library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bedf860-407d-44b7-9259-706fde161006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pinecone-client\n",
      "Version: 3.2.2\n",
      "Summary: Pinecone client and SDK\n",
      "Home-page: https://www.pinecone.io\n",
      "Author: Pinecone Systems, Inc.\n",
      "Author-email: support@pinecone.io\n",
      "License: Apache-2.0\n",
      "Location: C:\\Users\\njm_s\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\n",
      "Requires: certifi, tqdm, typing-extensions, urllib3\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pinecone-client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355bdc55-518a-4976-af9d-95e5a2bb2270",
   "metadata": {},
   "source": [
    "With the new client import the Pinecone class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10c45ace-dcb3-4cd9-aa8f-d1461d7d5756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 1536,\n",
       "              'host': 'langchain-qppbyn2.svc.gcp-starter.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'langchain',\n",
       "              'spec': {'pod': {'environment': 'gcp-starter',\n",
       "                               'pod_type': 'starter',\n",
       "                               'pods': 1,\n",
       "                               'replicas': 1,\n",
       "                               'shards': 1}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "pc = Pinecone()  #this constructor expects an environment variable called PINECONE_API_KEY. But if we've already created and loaded such a variable into memory in environment variable's file, then the authentication is automatically handeled. However, if you haven't loaded the environment variable, you should explicitly pass an argument named api_key with your key's value to the pinecone constructor.\n",
    "#pc = Pinecone(api_key='YOUR_API_KEY')\n",
    "\n",
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f8bdff-6a29-4590-9e3c-faeb1d52b771",
   "metadata": {},
   "source": [
    "## Working with Pinecone Indexes\n",
    "An **index** is the highest-level organizational unit of vector data in Pinecone. It accepts and stores vectors, serves queries over the vectors it contains, and does other vector operations over its contents.\n",
    "\n",
    "Currently, there are **two types of indexes:**\n",
    "\n",
    "**1)Serverless indexes:** You don't configure or manage any computing or storage resources(They scale automatically based on usage, and you pay only for the amount of data stored and operations performed with no minimums).\n",
    "\n",
    "**2)Pod-based indexes:** You choose one or more pre-configured units of hardware for running a pinecone service(pods). Depending on the pod type, pod size, and the number of pods used, you get a different amount of storage and higher or lower latency and throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e3ca103-7b6d-403b-b25b-b7256c6c091a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 1536,\n",
       "              'host': 'langchain-qppbyn2.svc.gcp-starter.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'langchain',\n",
       "              'spec': {'pod': {'environment': 'gcp-starter',\n",
       "                               'pod_type': 'starter',\n",
       "                               'pods': 1,\n",
       "                               'replicas': 1,\n",
       "                               'shards': 1}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes() # to get a complete description of all indexes in a project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcbe960-7510-4512-9fd6-75795e590beb",
   "metadata": {},
   "source": [
    " Note that the output is a list of dictionaries, and you can access the name of the first index as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27745302-bc97-4049-be72-8adbc4f1690f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'host': 'langchain-qppbyn2.svc.gcp-starter.pinecone.io',\n",
       " 'metric': 'cosine',\n",
       " 'name': 'langchain',\n",
       " 'spec': {'pod': {'environment': 'gcp-starter',\n",
       "                  'pod_type': 'starter',\n",
       "                  'pods': 1,\n",
       "                  'replicas': 1,\n",
       "                  'shards': 1}},\n",
       " 'status': {'ready': True, 'state': 'Ready'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9261f0f1-cd9b-470d-ad0f-77b36e716cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langchain'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes()[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2cfd387-0503-4280-9f17-74b0d5300358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'host': 'langchain-qppbyn2.svc.gcp-starter.pinecone.io',\n",
       " 'metric': 'cosine',\n",
       " 'name': 'langchain',\n",
       " 'spec': {'pod': {'environment': 'gcp-starter',\n",
       "                  'pod_type': 'starter',\n",
       "                  'pods': 1,\n",
       "                  'replicas': 1,\n",
       "                  'shards': 1}},\n",
       " 'status': {'ready': True, 'state': 'Ready'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.describe_index('langchain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d408bea3-eed4-49f4-a4aa-3074aab81668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['langchain']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes().names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35074f9c-6381-48b1-90e8-ec2a44507176",
   "metadata": {},
   "source": [
    "### Create a Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cdd0cbb-c974-49a9-8beb-1229361c7dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index langchain\n",
      "Index created! :)\n"
     ]
    }
   ],
   "source": [
    "from pinecone import PodSpec    #This represents the configuration used to deploy a pod-based index.\n",
    "index_name = 'langchain'                             # create a new index called \"langchain\"\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    print(f'Creating index {index_name}')\n",
    "    pc.create_index(\n",
    "        name = index_name,\n",
    "        dimension = 1536,  #This is the default dimension for text-embedding-3-small(one of the recommended OpenAI's embedding models.)\n",
    "        metric = 'cosine',  # This is the algorithm used to calculate the distance between vectors.\n",
    "        spec = PodSpec(\n",
    "            environment = 'gcp-starter'\n",
    "        ) \n",
    "    )\n",
    "    print('Index created! :)')\n",
    "else:\n",
    "     print(f'Index {index_name} already exists!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c7512-9de9-4a0f-a83e-a8e83e4f82c7",
   "metadata": {},
   "source": [
    "### Delete a Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ab043c9-1d3d-49ff-b401-1babe62a7edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting index langchain ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "index_name = 'langchain'\n",
    "if index_name in pc.list_indexes().names():\n",
    "    print(f'Deleting index {index_name} ...')\n",
    "    pc.delete_index(index_name)\n",
    "    print('Done')\n",
    "else:\n",
    "    print(f'Index {index_name} does not exists!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78245729-40a7-4b39-8f29-2e9b8f381cc6",
   "metadata": {},
   "source": [
    "**Important Note:** To perform any operation with an index, you must first select it. To select an index do the following:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90193caa-686e-49b2-bac7-a81ec092aa33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(index_name)  # this method will return an object of type index\n",
    "index.describe_index_stats()                  # Here I am selecting the index and displaying some statistics about it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2db1e3-6222-46d9-b27b-67026ae6fab4",
   "metadata": {},
   "source": [
    "**note:-** Serverless indexes automatically scale as needed, so index_fullness mainly applies to pod_based indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f60de-a7b5-4180-a2c5-d2e3a821632f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
